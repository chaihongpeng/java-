# 机械学习

## 回归

- 回归分析(Regression Analysis): 
  - 根据数据,确定两种或多种以上变量间相互依赖的定量关系
  - $y=f(x_1,x_2,\cdots,x_n)$  

- 回归的种类
  - 根据变量数:一元回归,多元回归
  - 根据函数关系:线性回归,非线性回归

### 线性回归

- 线性回归
  - 回归分析中,变量与因变量存在线性关系
  - $y=ax+b$ 
- 损失函数
  - $\min\{\dfrac{1}{2m}\sum\limits_{i=1}^m(y_i^{'}-y_i)^2\}$ 
  - $\dfrac{1}{2m}$是为了让方差求导时自动忽略掉常数
  - $J=\dfrac1{2m}\sum\limits^m_{i=1}(y_2^{'}-y_i)^2=\dfrac1{2m}\sum\limits^m_{i=1}(ax_i+b-y_i)^2=g(a,b)$ 
    - $其中x_i和y_i都是样本结果和样本参数,是已知的实数.而a,b则是需要求解的未知数$ 
    - 梯度下降法,寻找极小值的一种方法.
  - 模型评估
    - $y和y^{'}的均方误差(MSE)$
      $MSE=\dfrac1m\sum\limits^m_{i=1}(y_i^{'}-y_i)^2$ 
    - $R方值(R^2)$ 
      $R^2=1-\dfrac{\sum\limits^m_{i=1}(y^{'}_i-y_i)^2}{\sum\limits^m_{i=1}(y_i-\bar{y_i})^2}=1-\dfrac{MSE}{方差}$ 
    - $MES越小越好,R^2分数越接近1越好$ 

## 线性模型

## 逻辑回归

## K近邻算法(KNN)

- 欧氏距离
  - $二维距离公式:d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ 
  - $n维距离公式:d_{12}=\sqrt{\sum\limits^n_{k1}(x_{1k}-x_{2k})^2}$ 
- k近邻算法属于监督学习算法,kmeans是无监督学习算法

## 决策树

- 决策树构建
  - 利用信息熵原理选择信息熵最大的属性作为分类属性,递归拓展决策树的分支,完成决策树的构造

- 信息熵
  - 度量随机变量不确定性的指标,熵越大,变量的不确定性就越大
  - $假定当前样本集合D中第k类样本所占的比例为p_k,则D的信息熵为:$
    $Ent(D)=-\sum\limits^{|y|}_{k=1}p_k\log_2p_k$ 
  - $Ent(D)的值越小,变量的不确定性越小$ 

## 深度学习

## 贝叶斯分类

## 常用的聚类算法

- Meanshift均值漂移聚类
  - 算法流程
    - 在中心点一定区域内检索数据
    - 更新中心点
    - 重复流程到中心点稳定
  - 特点
    - 自动发现类别数量,不需要人工选择
    - 需要选择区域半径
- DBSCAN算法

### K均值算法

- 以空间中k个点为中心进行聚类,对最靠近他们的对象进行归类,是聚类算法中最为基础但也最为重要的算法

## 主成分分析

- PCA:数据姜维技术中,应用最多的方法

  - 寻找k(k<n)维新数据,使他反映事务的主要特征
  - 核心:在信息损失尽可能少的情况下,降低数据损失

- 主成分分析实质:

  - 3D到2D:在空间中建立一个平面,使所有点到平面的距离尽可能的小,点到平面的距离实际上就是损失的信息
  - 主成分分析就是建立一个更低维度的空间,让所有的元素都投影在这个低维空间上

  - 高纬度数据中,不同维度数据之间是具有很高的相关性,主成分分析的目的是将投影之后不同特征的数据尽可能分开,让维度间减少相关性

# 深度学习

## 多层感知器

