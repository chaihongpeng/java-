# 神经网络

- 神经网络(neural network)

- 神经元(neuron): 神经网络中最基本的成分

- 阈值(threshold): 神经元的点位超出了阈值, 它就会被激活

M-P神经元模型

1. 神经元接收到$n$个来自其他神经元的传递过来的输入信号
2. 这些输入信号将通过带权重的连接(connection)进行传递
3. 神经元接受到的总输入值将与神经元的阈值$\theta$进行比较
4. 然后通过"激活函数$f(x)$"(activation function)处理以产生神经元的输入

$$
y=f(\sum\limits^n_{i=1}w_ix_i-\theta)=f(w^\top x+b)
$$



其中$f(x)$是一个激活函数

常见的激活函数有

- 阶跃函数$sgn$
- $sigmoid$函数

它能将较大范围内变化的输入值挤压到(0, 1)输出范围内, 因此有时也称为"挤压函数"(squashing function).

# 感知机与多层网络

感知机只能解决线性可分的问题, 线性不可分的问题就无法使用感知机来实现

感知机的学习目标就是求得能对数据集T中的正负样本完全正确划分的超平面, 其中$w^\top x- \theta即为超平面方程$

法向量$w$垂直于超平面

法向量$w$和位移项$b$确定一个位移超平面

法向量$w$指向的那一半空间为正空间, 另一半为负空间

- 感知机(Perceptron)由两层神经元(neuron)组成

- 输入层接受外界的输入信号后传递给输出层
- 输出层是M-P神经元, 亦称为"阈值逻辑单元"(threshold logic unit)

- 感知机能够容易的实现逻辑与或非运

$$
t=f(\sum_{i=1}^nw_ix_i-\theta)=f(w^Tx)\\
f(n)=
\begin{cases}
+1 & if\ n\ge0\\
-1 & otherwise
\end{cases}\\
w=[w_1\ w_2\ \cdots\ w_n \ \theta]^T\\
x=[x_1\ x_2\ \cdots\ x_n\ {-1}]^T
$$

其中假定$f$就是阶跃函数

多层感知机表示与, 或, 非

- "与(AND)($\land$)": 令 $w_1=w_2=1,\theta=2$, 则 $y=f(x_1+x_2-2)$, 仅在$x_1=x_2=1$时, $y=1$ 
- "或(OR)($\lor$)": 令$w_1=w_2=1,\theta=0.5$, 则 $y=f(x_1+x_2-0.5)$, 当$x_1=1$或$x_2=1$时, $y=1$ 
- "非(NOT)($\lnot$)": 令$w_1=-0.6,w_2=0,\theta=-0.5$ , 则 $y=f(-0.6x_1+0.5)$, 当$x_1=1$时, $y=0$; 当$x_1=0$时, $y=1$ 

# 特殊到一般

- 更一般的, 给定训练数据集, 权重$w_i(i=1,2,\cdots,n)$以及$\theta$可以通过学习得到. 
- 阈值$\theta$可以看作一个固定输入值为$-1.0$的"哑结点"(dummy node)
- 所对应的连接权重$w_{n+1}$, 这样权重和阈值的学习就可以统一为权重的学习. 
- 感知机学习规则非常简单, 队训练样例(x, y)
- 若当前感知机的输出为$\hat{y}$, 则感知机权重将这样调整

$$
w_i\leftarrow w_i+\Delta w_i\\
\Delta w_i=\eta(y-\hat{y})x_i
$$

- $x_i$是$x$对应第$i$个输入神经元的分量
  - 其中$\eta \in (0,1)$称之为学习率(learning rate),$\eta$通常设置为一个小正数, 例如0.1
  - 若感知机对训练样例$(x,y)$预测正确, 即$\hat{y}=y$, 则感知机不发生变化, 否则根据错误的程度进行权重调整

MLP(FNN)前馈神经网络

XOR($\oplus$)异或: 相同为0, 不同为1

$XOR \to (\lnot x_1\land x_2)\lor (x_1 \land \lnot x_2)$ 

$f(f(x_1-x_2-0.5)+f(-x_1+x_2-0.5)-0.5)$, 当$x_1$与$x_2$取反的时候$y=1$ 



- 感知机只有输出层神经元进行激活函数处理, 即只拥有一层功能神经元(functional neuron), 学习能力非常有限
- 实际上, 与, 或, 非问题都是线性可分(linearly separalbe)的问题
- 若两类模式是线性可分的, 及存在一个线性超平面能将他们分开, 则感知机的学习过程一定会收敛(converge)而求得适当的权向量$w=(w_1;w_2;\cdots;w_{n+1})$
- 否则感知机将会发生震荡(fluctuation), $w$难以稳定下来, 而不能求得合适解

# 多层感知机

- 要解决非线性可分的问题, 需考虑多层功能神经元. 简单的两层感知机就能解决异或问题

- 输出层与输入层之间的一层神经元, 被称为隐层或隐含层, 隐藏层和输出层神经元都是拥有激活函数的功能神经元

- 更一般常见的神经网络, 每层神经元与下一层的神经元全互连, 神经元之间不存在同层连接, 也不存在跨层连接.

- 这样的神经网络结构通常称为"多层前馈神经网络"(multi-layper feedforward neurol newworks)

- 其中输入层神经元接受外界输入, 隐层与输出层神经元对信号进行加工, 最终结果由输出神经元输出; 换言之,输入神经元仅是接受输入, 不进行函数处理, 隐层与输出层包含功能神经元; 通常称为"两层网络". 为避免歧义, 此处称为"单隐层网络".

- 只需包含隐层, 即可称为多层网络.

- 神经网络的学习过程, 就是根据训练数据来调整神经元之间的"连接权"以及每个功能神经元的阈值;

- 神经网络"学"到的东西, 蕴含在连接权和阈值中

  



# 误差逆传播(BP)

1. 给定训练集$D=\{(x_1,y_1),(X_2,y_2),\cdots,(X_m,y_m)\},x_i\in R^d,y_i\in R^l$
2. $d$个输入神经元, $l$个输出神经元, $q$个隐藏神经元的多层前馈网络结构



$v_{ih}$: 输入层到隐藏层的权重

$\gamma_h$: 第$h$个隐藏神经元的阈值

$b_h$: 第$h$个隐藏神经元经过激活函数处理的输出

$w_hj$: 隐藏层和输出层的权重

$\beta_j$: 第$j$个输出神经元的输入

$\theta_j$: 第$j$个输出神经元的阈值

$\hat{y}_j$: 第$j$个输出神经元经过激活函数处理的输出