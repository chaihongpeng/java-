# 回归

- 回归分析(Regression Analysis): 
  - 根据数据,确定两种或多种以上变量间相互依赖的定量关系
  - $y=f(x_1,x_2,\cdots,x_n)$  

- 回归的种类
  - 根据变量数:一元回归,多元回归
  - 根据函数关系:线性回归,非线性回归

# 线性回归

正交回归: 样本点到线性回归方程的距离叫做正交回归

线性回归: 样本点到线性回归方程平行$y$轴的距离

均方误差: 样本点到回归方程的$y$轴距离的平方和叫做均方误差

预测误差: 没有加平方的距离和

$y=wx+b$

最小二乘法的损失函数: $E(w,b)=\sum\limits_{i=1}^{m}(y_i-f(x_i))^2=\sum\limits_{i=1}^m(y_i-y_i\prime)^2$

例:

身高与体重的预测: $f(x)=w_1x_1+b$

二值离散特征, 如颜值:好看1,不好看0

有序的多值离散特征: 如饭量: 小1,中2,大3

无序的多值离散特征, 就是把参数拆成多个二值特征: 如肤色: 黄[1,0,0],黑[0,1,0],白[0,0,1]

# 损失函数

# 最小二乘估计

基于均方误差最小化来进行模型求解的方法成为"最小二乘法"

均方误差公式:
$$
\begin{equation}
\begin{split}
E_{(w,b)}&=\sum\limits_{i=1}^m(y_i-f(x_i))^2\\
         &=\sum\limits_{i=1}^m(y_i-(wx_i+b))^2\\
         &=\sum\limits_{i=1}^m(y_i-wx_i-b)^2\\
\end{split}
\end{equation}
$$
最小二乘法公式:
$$
\mathop{\arg\min}\limits_{(w,b)}(E_{(w,b)})
$$

# 极大似然估计

估计概率分布的参数值

举例:

一个袋子里有无限个黑球和白球, 随机抽取了n次样本, 出现$m$次黑球, $n-m$次白球

假设黑球的概率为$\theta$, 白球的概率为$1-\theta$

发生此次事件的概率为$L(\theta)=n\theta\times(n-m)(1-\theta)$

极大似然认为存在及合理, 抽取样本出现的情况就是概率最大的情况

及求解$\theta$的问题转变成为求出$\theta$使得$L(\theta)$达到最大值点的问题, 进而使用导数求解

例如: 抽10次球, 7次黑球3次白球,那么我们认为这个装有无限多个球的袋子里, 黑球和白球的比例也是7: 3, 所以$\theta : (1-\theta)$近似等于$7:3$



$\ln{ab}=\ln{a}+\ln{b}$

$ln\prod\limits_{i=1}^nf(x_i)=\sum\limits_{i=1}^n\ln{f(x_i)}$

一般对于线性回归来说, 也可以假设一下模型:

$y=wx+b+\epsilon$

其中$\epsilon$是不受控制的误差, 通常其服从均值为0的正态分布$\epsilon\sim N(0,\sigma^2)$



对随机变量建模, 我们一般会先确认一下

- 线性回归
  - 回归分析中,变量与因变量存在线性关系
  - $y=ax+b$ 
- 损失函数
  - $\min\{\dfrac{1}{2m}\sum\limits_{i=1}^m(y_i^{'}-y_i)^2\}$ 
  - $\dfrac{1}{2m}$是为了让方差求导时自动忽略掉常数
  - $J=\dfrac1{2m}\sum\limits^m_{i=1}(y_2^{'}-y_i)^2=\dfrac1{2m}\sum\limits^m_{i=1}(ax_i+b-y_i)^2=g(a,b)$ 
    - $其中x_i和y_i都是样本结果和样本参数,是已知的实数.而a,b则是需要求解的未知数$ 
    - 梯度下降法,寻找极小值的一种方法.
  - 模型评估
    - $y和y^{'}的均方误差(MSE)$
      $MSE=\dfrac1m\sum\limits^m_{i=1}(y_i^{'}-y_i)^2$ 
    - $R方值(R^2)$ 
      $R^2=1-\dfrac{\sum\limits^m_{i=1}(y^{'}_i-y_i)^2}{\sum\limits^m_{i=1}(y_i-\bar{y_i})^2}=1-\dfrac{MSE}{方差}$ 
    - $MES越小越好,R^2分数越接近1越好$ 