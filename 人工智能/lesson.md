- 数据集(data set): 一组西瓜
- 示例(instance)/样本(sample): 一个西瓜
- 属性(attribute)/特征(feature): 色泽, 声响; key
- 属性值(attribute value): 绿色, 清脆; value
- 属性空间(attribute space)/样本空间(sample space)/输入空间: 多属性形成一个坐标轴, 每一个西瓜在坐标轴上都有自己的坐标位置

> 由于空间中每一个点对应一个坐标向量, 因此我们把一个示例称为一个特征向量(feature vector)

- 一般的令$D=\{x_1,x_2,\cdots,x_m\}$表示有$m$个示例的数据集
  - $D$代表数据集$data\ set$
  - $x$代表示例
- 示例由$d$个属性描述, 每个示例$x_i=(x_{i1};x_{i2};\cdots;x_{i_d})$是$d$维样本空间$X$中的一个向量,$x\in X$
  - $d$代表维数(dimensionality)
  - $x_{i}$代表示例(instance/sample)
- 其中$x_{ij}$是$x_i$在第$j$个属性上的取值



- 学习(learning)/训练(training): 从数据中学得模型的过程
- 训练数据(training data): 训练过程中使用的数据
- 训练样本(training sample): 训练数据中的每一个样本成为训练样本
- 训练集合(training set): 训练样本组成的集合称为训练集
- 假设(hypothesis): 学得模型对应了关于数据的某种潜在规律
- 真相/真实(ground-truth): 这种潜在规律称为真相或真实
- 学习器(learner): 模型
- 预测(prediction)
- 标记(label): 事例的结果
- 样例(example): 拥有标记信息的示例
- 一般用$(x_i,y_i)$表示第$i$个样例, 其中$y_i\in Y$表示是示例$x_i$的标记,$Y$是所有标记的集合
- 标记空间(label space)/输出空间: 所有标记集合



- 分类(classification): 预测离散值的学习任务
  - 二分类(binary classification): 只涉及两个类别, 一般为"正类"和"负/反类"
  - 多分类(multi-class classification): 涉及多个类别
- 回归(regression): 预测连续值的学习任务



- 聚类(clustering): 对训练集进行分组
- 组(cluster): 每个组



- 监督学习(supervised learning)
- 无监督学习(unsupervised learning)



- 泛化(generalization): 学得模型适用于新样本的能力



## 假设空间

科学推理的两大基本手段

- 归纳(induction):从特殊到一般的泛化过程, 及从具体的事实归结出一般性规律
- 演绎(deduction): 从一般到特殊的"特化"过程, 及从基本原理推演出具体状况



- 归纳学习(inductive learning): 从样本中学习是一个归纳的过程, 因此亦称为归纳学习
    - 广义归纳学习: 从样例中学习
    - 狭义归纳学习: 从训练数据中学得概念(concept), 因此亦称为"概念学习或"概念形成"

我们把学习过程看作一个在所有假设(hypothesis)组成的空间中进行搜索的过程,  搜索目标是找到与训练集"匹配"(fit)的假设, 即能够将训练集中的瓜判断正确的假设

可能有多个假设与训练集一致, 即存在一个与训练集一直的"假设集合", 亦称为"版本空间"

## 线性模型

给定$d$个属性描述示例$x=(x_1;x_2;\cdots;x_i)$其中$x_i$是$x$在第$i$个属性上的取值

## 逻辑回归



## 决策树

信息熵 $Ent(D)=-\sum\limits_{k=1}^{|Y|}p_k\log_2p_k$

## 神经网络

- 神经网络(neural network)

- 神经元(neuron): 神经网络中最基本的成分

- 阈值(threshold): 神经元的点位超出了阈值, 它就会被激活

M-P神经元模型

1. 神经元接收到n个来自其他神经元的传递过来的输入信号
2. 这些输入信号将通过带权重的连接(connection)进行传递
3. 神经元接受到的总输入值将与神经元的阈值进行比较
4. 然后通过"激活函数"(activation function)处理以产生神经元的输入

阶跃函数, 常使用$Sigmoid$函数作为激活函数, 它能将较大范围内变化的输入值挤压到(0, 1)输出范围内, 因此有时也称为"挤压函数"(squashing function).

## 感知机与多层网络

- 感知机(Perceptron)由两层神经元(neuron)组成

- 输入层接受外界的输入信号后传递给输出层
- 输出层是M-P神经元, 亦称为"阈值逻辑单元"(threshold logic unit)

- 感知机能够容易的实现逻辑与或非运

$$
t=f(\sum_{i=1}^nw_ix_i-\theta)=f(w^Tx)\\
f(n)=
\begin{cases}
+1 & if\ n\ge0\\
-1 & otherwise
\end{cases}\\
w=[w_1\ w_2\ \cdots\ w_n \ \theta]^T\\
x=[x_1\ x_2\ \cdots\ x_n\ {-1}]^T
$$

其中假定$f$就是阶跃函数

多层感知机表示与, 或, 非

- "与(AND)($\land$)": 令 $w_1=w_2=1,\theta=2$, 则 $y=f(x_1+x_2-2)$, 仅在$x_1=x_2=1$时, $y=1$ 
- "或(OR)($\lor$)": 令$w_1=w_2=1,\theta=0.5$, 则 $y=f(x_1+x_2-0.5)$, 当$x_1=1$或$x_2=1$时, $y=1$ 
- "非(NOT)($\lnot$)": 令$w_1=-0.6,w_2=0,\theta=-0.5$ , 则 $y=f(-0.6x_1+0.5)$, 当$x_1=1$时, $y=0$; 当$x_1=0$时, $y=1$ 

### 特殊到一般

- 更一般的, 给定训练数据集, 权重$w_i(i=1,2,\cdots,n)$以及$\theta$可以通过学习得到. 
- 阈值$\theta$可以看作一个固定输入值为$-1.0$的"哑结点"(dummy node)
- 所对应的连接权重$w_{n+1}$, 这样权重和阈值的学习就可以统一为权重的学习. 
- 感知机学习规则非常简单, 队训练样例(x, y)
-  若当前感知机的输出为$\hat{y}$, 则感知机权重将这样调整

$$
w_i\leftarrow w_i+\Delta w_i\\
\Delta w_i=\eta(y-\hat{y})x_i
$$
- $x_i$是$x$对应第$i$个输入神经元的分量
  - 其中$\eta \in (0,1)$称之为学习率(learning rate),$\eta$通常设置为一个小正数, 例如0.1
  - 若感知机对训练样例$(x,y)$预测正确, 即$\hat{y}=y$, 则感知机不发生变化, 否则根据错误的程度进行权重调整

MLP(FNN)前馈神经网络

XOR($\oplus$)异或: 相同为0, 不同为1

$XOR \to (\lnot x_1\land x_2)\lor (x_1 \land \lnot x_2)$ 

$f(f(x_1-x_2-0.5)+f(-x_1+x_2-0.5)-0.5)$, 当$x_1$与$x_2$取反的时候$y=1$ 



- 感知机只有输出层神经元进行激活函数处理, 即只拥有一层功能神经元(functional neuron), 学习能力非常有限
- 实际上, 与, 或, 非问题都是线性可分(linearly separalbe)的问题
- 若两类模式是线性可分的, 及存在一个线性超平面能将他们分开, 则感知机的学习过程一定会收敛(converge)而求得适当的权向量$w=(w_1;w_2;\cdots;w_{n+1})$
- 否则感知机将会发生震荡(fluctuation), $w$难以稳定下来, 而不能求得合适解

## 多层感知机

- 要解决非线性可分的问题, 需考虑多层功能神经元. 简单的两层感知机就能解决异或问题

- 输出层与输入层之间的一层神经元, 被称为隐层或隐含层, 隐藏层和输出层神经元都是拥有激活函数的功能神经元

- 更一般常见的神经网络, 每层神经元与下一层的神经元全互连, 神经元之间不存在同层连接, 也不存在跨层连接.

- 这样的神经网络结构通常称为"多层前馈神经网络"(multi-layper feedforward neurol newworks)

- 其中输入层神经元接受外界输入, 隐层与输出层神经元对信号进行加工, 最终结果由输出神经元输出; 换言之,输入神经元仅是接受输入, 不进行函数处理, 隐层与输出层包含功能神经元; 通常称为"两层网络". 为避免歧义, 此处称为"单隐层网络".

- 只需包含隐层, 即可称为多层网络.

- 神经网络的学习过程, 就是根据训练数据来调整神经元之间的"连接权"以及每个功能神经元的阈值;

- 神经网络"学"到的东西, 蕴含在连接权和阈值中

  



## 误差逆传播(BP)

1. 给定训练集$D=\{(x_1,y_1),(X_2,y_2),\cdots,(X_m,y_m)\},x_i\in R^d,y_i\in R^l$
2. $d$个输入神经元, $l$个输出神经元, $q$个隐藏神经元的多层前馈网络结构



$v_{ih}$: 输入层到隐藏层的权重

$\gamma_h$: 第$h$个隐藏神经元的阈值

$b_h$: 第$h$个隐藏神经元经过激活函数处理的输出

$w_hj$: 隐藏层和输出层的权重

$\beta_j$: 第$j$个输出神经元的输入

$\theta_j$: 第$j$个输出神经元的阈值

$\hat{y}_j$: 第$j$个输出神经元经过激活函数处理的输出





