

# 回归

- 回归分析(Regression Analysis): 
  - 根据数据,确定两种或多种以上变量间相互依赖的定量关系
  - $y=f(x_1,x_2,\cdots,x_n)$  

- 回归的种类
  - 根据变量数:一元回归,多元回归
  - 根据函数关系:线性回归,非线性回归

# 线性回归

正交回归: 样本点到线性回归方程的距离叫做正交回归

线性回归: 样本点到线性回归方程平行$y$轴的距离

均方误差: 样本点到回归方程的$y$轴距离的平方和叫做均方误差

预测误差: 没有加平方的距离和

$y=wx+b$

最小二乘法的损失函数: $E(w,b)=\sum\limits_{i=1}^{m}(y_i-f(x_i))^2=\sum\limits_{i=1}^m(y_i-y_i\prime)^2$

例:

身高与体重的预测: $f(x)=w_1x_1+b$

二值离散特征, 如颜值:好看1,不好看0

有序的多值离散特征: 如饭量: 小1,中2,大3

无序的多值离散特征, 就是把参数拆成多个二值特征: 如肤色: 黄[1,0,0],黑[0,1,0],白[0,0,1]

# 最小二乘估计

$\arg\min\limits_{(w,b)}(E(w,b))$

# 极大自然估计

估计概率分布的参数值

技巧: 使用$\ln$函数特性将连乘转化为连加

$\ln{ab}=\ln{a}+\ln{b}$

$ln\prod\limits_{i=1}^nf(x_i)=\sum\limits_{i=1}^n\ln{f(x_i)}$

一般对于线性回归来说, 也可以假设一下模型:

$y=wx+b+\epsilon$

其中$\epsilon$是不受控制的误差, 通常其服从均值为0的正态分布$\epsilon\sim N(0,\sigma^2)$



对随机变量建模, 我们一般会先确认一下

- 线性回归
  - 回归分析中,变量与因变量存在线性关系
  - $y=ax+b$ 
- 损失函数
  - $\min\{\dfrac{1}{2m}\sum\limits_{i=1}^m(y_i^{'}-y_i)^2\}$ 
  - $\dfrac{1}{2m}$是为了让方差求导时自动忽略掉常数
  - $J=\dfrac1{2m}\sum\limits^m_{i=1}(y_2^{'}-y_i)^2=\dfrac1{2m}\sum\limits^m_{i=1}(ax_i+b-y_i)^2=g(a,b)$ 
    - $其中x_i和y_i都是样本结果和样本参数,是已知的实数.而a,b则是需要求解的未知数$ 
    - 梯度下降法,寻找极小值的一种方法.
  - 模型评估
    - $y和y^{'}的均方误差(MSE)$
      $MSE=\dfrac1m\sum\limits^m_{i=1}(y_i^{'}-y_i)^2$ 
    - $R方值(R^2)$ 
      $R^2=1-\dfrac{\sum\limits^m_{i=1}(y^{'}_i-y_i)^2}{\sum\limits^m_{i=1}(y_i-\bar{y_i})^2}=1-\dfrac{MSE}{方差}$ 
    - $MES越小越好,R^2分数越接近1越好$ 

### 逻辑回归



## K近邻算法(KNN)

- 欧氏距离
  - $二维距离公式:d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ 
  - $n维距离公式:d_{12}=\sqrt{\sum\limits^n_{k1}(x_{1k}-x_{2k})^2}$ 
- k近邻算法属于监督学习算法,kmeans是无监督学习算法
- k近邻算法的弊端, 它在识别图像时无法区分主体成分和背景, 因此k近邻会把背景相似或形状相似的图片划分为同一类别

## 决策树

- 决策树构建
  - 利用信息熵原理选择信息熵最大的属性作为分类属性,递归拓展决策树的分支,完成决策树的构造

- 信息熵
  - 度量随机变量不确定性的指标,熵越大,变量的不确定性就越大
  - $假定当前样本集合D中第k类样本所占的比例为p_k,则D的信息熵为:$
    $Ent(D)=-\sum\limits^{|y|}_{k=1}p_k\log_2p_k$ 
  - $Ent(D)的值越小,变量的不确定性越小$ 

## 深度学习

## 贝叶斯分类

## 常用的聚类算法

- Meanshift均值漂移聚类
  - 算法流程
    - 在中心点一定区域内检索数据
    - 更新中心点
    - 重复流程到中心点稳定
  - 特点
    - 自动发现类别数量,不需要人工选择
    - 需要选择区域半径
- DBSCAN算法

### K均值算法

- 以空间中k个点为中心进行聚类,对最靠近他们的对象进行归类,是聚类算法中最为基础但也最为重要的算法

## 主成分分析

- PCA:数据姜维技术中,应用最多的方法

  - 寻找k(k<n)维新数据,使他反映事务的主要特征
  - 核心:在信息损失尽可能少的情况下,降低数据损失

- 主成分分析实质:

  - 3D到2D:在空间中建立一个平面,使所有点到平面的距离尽可能的小,点到平面的距离实际上就是损失的信息
  - 主成分分析就是建立一个更低维度的空间,让所有的元素都投影在这个低维空间上

  - 高纬度数据中,不同维度数据之间是具有很高的相关性,主成分分析的目的是将投影之后不同特征的数据尽可能分开,让维度间减少相关性

# 深度学习

## 多层感知器

# 损失函数

$L_i=\sum_{j\neq y}\max(0,s_j-s_{y_j}+\Delta)$

$s_j$是正确类别的得分

$s_{y_j}$是第$i$个错误类别的得分

$\Delta$代表容忍程度, 正确结果必须要比错误结果高出$\Delta$个数值才被认为能够区分出数据
